{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRBoost Default Usage\n",
    "- LRBoost defaults to RidgeCV and HistGradientBoostingRegressor as the primary and secondary models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrboost import LRBoostRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "lrb = LRBoostRegressor().fit(X, y)\n",
    "preds = lrb.predict(X)\n",
    "#BUG -- need to fix ridge errors here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly Provide Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrboost import LRBoostRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "ridge_args = {\"alphas\": np.logspace(-4, 3, 10, endpoint=True), \"cv\": 5}\n",
    "rf_args = {\"n_estimators\": 50, \"n_jobs\": -1}\n",
    "lrb = LRBoostRegressor(primary_model=RidgeCV(**ridge_args),\n",
    "                    secondary_model=RandomForestRegressor(**rf_args))\n",
    "lrb = lrb.fit(X, y)\n",
    "preds = lrb.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparamters\n",
    "\n",
    "- Note that when doing a tuning search such asn RandomSearchCV(), the primary model cannot be also a CV'd model. Therefore we replace RidgeCV() with Ridge().\n",
    "- When creating the parameter grids, ensure to follow the primary_model___ and secondary_model___ syntax. The grid provided is particular to LightGBM and would need to be adjusted for XGBoost, Catboost, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from lightgbm import LGBMRegressor\n",
    "from lrboost import LRBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "X_train = X[0:140, ]\n",
    "X_val = X[140:150, ]\n",
    "y_train = y[0:140]\n",
    "y_val = y[140:150]\n",
    "\n",
    "fit_params = {\n",
    "    \"early_stopping_rounds\": 3, \n",
    "    \"eval_metric\": 'rmse', \n",
    "    \"eval_set\": [(X_val, y_val)],\n",
    "    \"eval_names\": ['validation'],\n",
    "    \"verbose\": 100\n",
    "    }\n",
    "\n",
    "lrb = LRBoostRegressor(primary_model=Ridge(),\n",
    "                        secondary_model=LGBMRegressor())\n",
    "\n",
    "param_grid = {\n",
    "    'primary_model__alpha': np.logspace(-4, 3, 10, endpoint=True), \n",
    "    'secondary_model__num_leaves': randint(6, 50), \n",
    "    'secondary_model__min_child_samples': randint(100, 500), \n",
    "    'secondary_model__min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "    'secondary_model__learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 100)),\n",
    "    'secondary_model__subsample': uniform(loc = 0.2, scale = 0.8), \n",
    "    'secondary_model__colsample_bytree': uniform(loc = 0.4, scale = 0.6),\n",
    "    'secondary_model__reg_alpha': [0, 1e-1, 1, 2, 10, 100],\n",
    "    'secondary_model__reg_lambda': [0, 1e-1, 1, 2, 10, 100]\n",
    "    }\n",
    "        \n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator = lrb, \n",
    "    param_distributions = param_grid)\n",
    "\n",
    "rand_search = rand_search.fit(X_train, y_train, secondary_fit_params = fit_params)\n",
    "\n",
    "best_model = rand_search.best_estimator_\n",
    "\n",
    "preds = best_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Secondary Models\n",
    "- The use of models such as NGBoost or XGBoostDistribution result in probabilistic predictions with multiple parameters.\n",
    "- Several methods for obtaining the predictions are provided below.\n",
    "- *BE EXTREMELY CAUTIOUS WITH THE INTERPRETATION OF THE MODEL VARIANCE AS IT IS ONLY DIRECTLY APPLIED TO THE SECONDARY PREDICTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO fill in after LRBoostRegressorDist is completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tester')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57a20780c7a63fbb8aabf167ed687f261b7c1c236162e2ad77ba218a57790661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
